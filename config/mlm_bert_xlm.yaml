project_root: /mounts/u-amo-d1/adibm-data/projects/latin
pretrain_data:
  - 
    - data/cltk/lat/text/lat_text_latin_library
    - .txt
    - 1
data_path: data/
# num_files: 50
num_files : None
save_steps : 0
eval_steps : 50
bert_path: models/latin_bert
tokenizer_path: models/latin_bert/latin.subword.encoder
train_batch_size: 6
eval_batch_size: 10
# eval_batch_size: 60
gradient_accumulation_steps: 32
model_name: silencesys/paraphrase-xlm-r-multilingual-v1-fine-tuned-for-medieval-latin
# model_name: outputs/2024-02-27/23-01-39/pretrain
max_length: 512
mlm_probability: 0.15
fp16: True
early_stopping_patience: 4
train_epochs: 50
pretrain_epochs: 50
data_split_percentage: 1
pretrain_out_path: "pretrain"
train_out_path: "train"